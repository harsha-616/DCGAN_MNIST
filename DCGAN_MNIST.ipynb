{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms,datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "hidden_dim = 64\n",
        "image_dim = 3  # CIFAR-10: 3 channels (RGB)\n",
        "num_epochs = 150\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.noise_dim = noise_dim\n",
        "\n",
        "        # Initial dense layer to create 7x7x128 feature map\n",
        "        self.fc = nn.Linear(noise_dim, 7 * 7 * 128)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Upsample to 14x14x64\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Upsample to 28x28x32\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Final layer to 28x28x1\n",
        "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()  # Output in [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, noise_dim]\n",
        "        x = self.fc(x)  # [batch_size, 5*5*512]\n",
        "        x = x.view(-1, 128, 7, 7)  # Reshape to [batch_size, 512, 5, 5]\n",
        "        return self.model(x)  # Pass through convolutional layers\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # [batch_size, 64, 100, 100]\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # [batch_size, 128, 50, 50]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, 1)  # Logit output for real/fake\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Data loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # [0, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # [-1, 1]\n",
        "])\n",
        "dataset = datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, _) in enumerate(dataloader):  # Unpack tuple, ignore labels\n",
        "        real_images = real_images.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # Labels\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        d_optimizer.zero_grad()\n",
        "        real_output = discriminator(real_images)\n",
        "        d_real_loss = criterion(real_output, real_labels)\n",
        "        noise = torch.randn(batch_size, 100).to(device)\n",
        "        fake_images = generator(noise)\n",
        "        fake_output = discriminator(fake_images.detach())\n",
        "        d_fake_loss = criterion(fake_output, fake_labels)\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        g_optimizer.zero_grad()\n",
        "        fake_output = discriminator(fake_images)\n",
        "        g_loss = criterion(fake_output, real_labels)\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(dataloader)}] \"\n",
        "                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "       # Inside the training loop, after each epoch\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save(generator.state_dict(), f'generator_epoch_{epoch}.pth')\n",
        "        torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch}.pth')\n",
        "\n",
        "    # Save generated images\n",
        "    if epoch % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            fake_images = generator(torch.randn(16, 100).to(device))\n",
        "            fake_images = fake_images * 0.5 + 0.5  # Denormalize to [0, 1]\n",
        "            fake_images = fake_images.permute(0, 2, 3, 1).cpu().numpy()\n",
        "            fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "            for i, ax in enumerate(axes.flat):\n",
        "                ax.imshow(fake_images[i])\n",
        "                ax.axis('off')\n",
        "            plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n",
        "            plt.close()"
      ],
      "metadata": {
        "id": "nYy44R8zGRsW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}