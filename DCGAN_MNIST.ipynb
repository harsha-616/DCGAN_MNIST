{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V5TfH7rJqmG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20d45716-8c40-401f-9c5b-666de8c3e080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/150] Batch [0/469] D Loss: 1.3041, G Loss: 0.9106\n",
            "Epoch [0/150] Batch [100/469] D Loss: 0.7454, G Loss: 1.4936\n",
            "Epoch [0/150] Batch [200/469] D Loss: 1.0467, G Loss: 1.0422\n",
            "Epoch [0/150] Batch [300/469] D Loss: 1.0409, G Loss: 1.0112\n",
            "Epoch [0/150] Batch [400/469] D Loss: 0.8429, G Loss: 1.2907\n",
            "Epoch [1/150] Batch [0/469] D Loss: 0.7475, G Loss: 1.7775\n",
            "Epoch [1/150] Batch [100/469] D Loss: 0.7041, G Loss: 1.1943\n",
            "Epoch [1/150] Batch [200/469] D Loss: 0.7691, G Loss: 1.2750\n",
            "Epoch [1/150] Batch [300/469] D Loss: 0.8212, G Loss: 1.6103\n",
            "Epoch [1/150] Batch [400/469] D Loss: 0.8165, G Loss: 1.3937\n",
            "Epoch [2/150] Batch [0/469] D Loss: 0.8964, G Loss: 0.9299\n",
            "Epoch [2/150] Batch [100/469] D Loss: 0.8305, G Loss: 1.2227\n",
            "Epoch [2/150] Batch [200/469] D Loss: 0.9446, G Loss: 0.9779\n",
            "Epoch [2/150] Batch [300/469] D Loss: 0.9003, G Loss: 1.3757\n",
            "Epoch [2/150] Batch [400/469] D Loss: 0.9255, G Loss: 1.2432\n",
            "Epoch [3/150] Batch [0/469] D Loss: 0.9979, G Loss: 0.8369\n",
            "Epoch [3/150] Batch [100/469] D Loss: 0.9981, G Loss: 1.4031\n",
            "Epoch [3/150] Batch [200/469] D Loss: 0.8964, G Loss: 1.2205\n",
            "Epoch [3/150] Batch [300/469] D Loss: 1.1024, G Loss: 0.8135\n",
            "Epoch [3/150] Batch [400/469] D Loss: 0.9227, G Loss: 1.0602\n",
            "Epoch [4/150] Batch [0/469] D Loss: 0.9496, G Loss: 1.4717\n",
            "Epoch [4/150] Batch [100/469] D Loss: 0.9588, G Loss: 1.2078\n",
            "Epoch [4/150] Batch [200/469] D Loss: 1.0814, G Loss: 1.7646\n",
            "Epoch [4/150] Batch [300/469] D Loss: 0.9588, G Loss: 0.8491\n",
            "Epoch [4/150] Batch [400/469] D Loss: 0.9547, G Loss: 1.0915\n",
            "Epoch [5/150] Batch [0/469] D Loss: 0.9315, G Loss: 1.1765\n",
            "Epoch [5/150] Batch [100/469] D Loss: 0.8905, G Loss: 1.1351\n",
            "Epoch [5/150] Batch [200/469] D Loss: 0.8529, G Loss: 1.3018\n",
            "Epoch [5/150] Batch [300/469] D Loss: 0.8906, G Loss: 0.9769\n",
            "Epoch [5/150] Batch [400/469] D Loss: 1.1425, G Loss: 1.6787\n",
            "Epoch [6/150] Batch [0/469] D Loss: 0.9703, G Loss: 1.1591\n",
            "Epoch [6/150] Batch [100/469] D Loss: 1.0443, G Loss: 1.2694\n",
            "Epoch [6/150] Batch [200/469] D Loss: 1.1900, G Loss: 1.5288\n",
            "Epoch [6/150] Batch [300/469] D Loss: 1.0646, G Loss: 0.9467\n",
            "Epoch [6/150] Batch [400/469] D Loss: 0.9795, G Loss: 1.1464\n",
            "Epoch [7/150] Batch [0/469] D Loss: 1.0600, G Loss: 1.3282\n",
            "Epoch [7/150] Batch [100/469] D Loss: 1.0227, G Loss: 1.5217\n",
            "Epoch [7/150] Batch [200/469] D Loss: 1.1183, G Loss: 1.2183\n",
            "Epoch [7/150] Batch [300/469] D Loss: 1.0162, G Loss: 1.2400\n",
            "Epoch [7/150] Batch [400/469] D Loss: 1.0802, G Loss: 1.1050\n",
            "Epoch [8/150] Batch [0/469] D Loss: 1.0219, G Loss: 1.0803\n",
            "Epoch [8/150] Batch [100/469] D Loss: 1.1943, G Loss: 1.5859\n",
            "Epoch [8/150] Batch [200/469] D Loss: 1.0295, G Loss: 0.9219\n",
            "Epoch [8/150] Batch [300/469] D Loss: 1.0976, G Loss: 1.0139\n",
            "Epoch [8/150] Batch [400/469] D Loss: 1.0878, G Loss: 1.0603\n",
            "Epoch [9/150] Batch [0/469] D Loss: 1.0263, G Loss: 0.8794\n",
            "Epoch [9/150] Batch [100/469] D Loss: 1.0446, G Loss: 1.4973\n",
            "Epoch [9/150] Batch [200/469] D Loss: 1.0550, G Loss: 0.9329\n",
            "Epoch [9/150] Batch [300/469] D Loss: 1.1600, G Loss: 1.2868\n",
            "Epoch [9/150] Batch [400/469] D Loss: 1.0675, G Loss: 1.2464\n",
            "Epoch [10/150] Batch [0/469] D Loss: 1.0498, G Loss: 1.4684\n",
            "Epoch [10/150] Batch [100/469] D Loss: 0.9957, G Loss: 1.2646\n",
            "Epoch [10/150] Batch [200/469] D Loss: 1.1041, G Loss: 1.3818\n",
            "Epoch [10/150] Batch [300/469] D Loss: 1.0785, G Loss: 1.0506\n",
            "Epoch [10/150] Batch [400/469] D Loss: 1.1217, G Loss: 0.9890\n",
            "Epoch [11/150] Batch [0/469] D Loss: 1.0162, G Loss: 1.0895\n",
            "Epoch [11/150] Batch [100/469] D Loss: 1.1883, G Loss: 0.7257\n",
            "Epoch [11/150] Batch [200/469] D Loss: 1.0756, G Loss: 0.8823\n",
            "Epoch [11/150] Batch [300/469] D Loss: 1.1005, G Loss: 1.1568\n",
            "Epoch [11/150] Batch [400/469] D Loss: 1.0409, G Loss: 1.0606\n",
            "Epoch [12/150] Batch [0/469] D Loss: 1.2410, G Loss: 0.7528\n",
            "Epoch [12/150] Batch [100/469] D Loss: 1.0627, G Loss: 1.3720\n",
            "Epoch [12/150] Batch [200/469] D Loss: 1.1705, G Loss: 0.7151\n",
            "Epoch [12/150] Batch [300/469] D Loss: 1.1391, G Loss: 1.3691\n",
            "Epoch [12/150] Batch [400/469] D Loss: 1.1628, G Loss: 0.7927\n",
            "Epoch [13/150] Batch [0/469] D Loss: 1.2160, G Loss: 1.3323\n",
            "Epoch [13/150] Batch [100/469] D Loss: 1.1862, G Loss: 0.6469\n",
            "Epoch [13/150] Batch [200/469] D Loss: 1.1255, G Loss: 0.7918\n",
            "Epoch [13/150] Batch [300/469] D Loss: 1.1583, G Loss: 0.9270\n",
            "Epoch [13/150] Batch [400/469] D Loss: 1.1079, G Loss: 1.1010\n",
            "Epoch [14/150] Batch [0/469] D Loss: 1.0809, G Loss: 1.2522\n",
            "Epoch [14/150] Batch [100/469] D Loss: 0.9661, G Loss: 0.9319\n",
            "Epoch [14/150] Batch [200/469] D Loss: 1.0563, G Loss: 1.4945\n",
            "Epoch [14/150] Batch [300/469] D Loss: 1.0508, G Loss: 1.3786\n",
            "Epoch [14/150] Batch [400/469] D Loss: 1.1122, G Loss: 1.0586\n",
            "Epoch [15/150] Batch [0/469] D Loss: 1.1298, G Loss: 1.0255\n",
            "Epoch [15/150] Batch [100/469] D Loss: 1.0043, G Loss: 0.9146\n",
            "Epoch [15/150] Batch [200/469] D Loss: 1.0426, G Loss: 1.0306\n",
            "Epoch [15/150] Batch [300/469] D Loss: 1.0944, G Loss: 0.9620\n",
            "Epoch [15/150] Batch [400/469] D Loss: 1.0950, G Loss: 0.7916\n",
            "Epoch [16/150] Batch [0/469] D Loss: 1.2045, G Loss: 0.6540\n",
            "Epoch [16/150] Batch [100/469] D Loss: 1.2983, G Loss: 0.5932\n",
            "Epoch [16/150] Batch [200/469] D Loss: 1.1006, G Loss: 1.1694\n",
            "Epoch [16/150] Batch [300/469] D Loss: 1.1555, G Loss: 0.7255\n",
            "Epoch [16/150] Batch [400/469] D Loss: 1.2859, G Loss: 1.1935\n",
            "Epoch [17/150] Batch [0/469] D Loss: 1.0808, G Loss: 0.8140\n",
            "Epoch [17/150] Batch [100/469] D Loss: 1.2383, G Loss: 0.7734\n",
            "Epoch [17/150] Batch [200/469] D Loss: 1.3824, G Loss: 1.6367\n",
            "Epoch [17/150] Batch [300/469] D Loss: 1.0320, G Loss: 0.9345\n",
            "Epoch [17/150] Batch [400/469] D Loss: 1.2345, G Loss: 1.4016\n",
            "Epoch [18/150] Batch [0/469] D Loss: 1.1159, G Loss: 0.9977\n",
            "Epoch [18/150] Batch [100/469] D Loss: 1.0049, G Loss: 1.2034\n",
            "Epoch [18/150] Batch [200/469] D Loss: 1.2062, G Loss: 0.9855\n",
            "Epoch [18/150] Batch [300/469] D Loss: 1.1150, G Loss: 0.8641\n",
            "Epoch [18/150] Batch [400/469] D Loss: 1.0670, G Loss: 0.9294\n",
            "Epoch [19/150] Batch [0/469] D Loss: 1.0850, G Loss: 1.1049\n",
            "Epoch [19/150] Batch [100/469] D Loss: 1.0858, G Loss: 1.0619\n",
            "Epoch [19/150] Batch [200/469] D Loss: 1.2461, G Loss: 0.5943\n",
            "Epoch [19/150] Batch [300/469] D Loss: 1.1243, G Loss: 1.3451\n",
            "Epoch [19/150] Batch [400/469] D Loss: 1.1287, G Loss: 1.1795\n",
            "Epoch [20/150] Batch [0/469] D Loss: 1.1950, G Loss: 0.7693\n",
            "Epoch [20/150] Batch [100/469] D Loss: 1.1415, G Loss: 1.2566\n",
            "Epoch [20/150] Batch [200/469] D Loss: 1.1217, G Loss: 0.9073\n",
            "Epoch [20/150] Batch [300/469] D Loss: 1.1486, G Loss: 0.8156\n",
            "Epoch [20/150] Batch [400/469] D Loss: 1.1978, G Loss: 0.7179\n",
            "Epoch [21/150] Batch [0/469] D Loss: 1.1481, G Loss: 0.9484\n",
            "Epoch [21/150] Batch [100/469] D Loss: 1.1761, G Loss: 1.4104\n",
            "Epoch [21/150] Batch [200/469] D Loss: 1.2201, G Loss: 0.7357\n",
            "Epoch [21/150] Batch [300/469] D Loss: 1.1796, G Loss: 0.8881\n",
            "Epoch [21/150] Batch [400/469] D Loss: 1.2090, G Loss: 0.9303\n",
            "Epoch [22/150] Batch [0/469] D Loss: 1.1793, G Loss: 0.6564\n",
            "Epoch [22/150] Batch [100/469] D Loss: 1.2030, G Loss: 1.0594\n",
            "Epoch [22/150] Batch [200/469] D Loss: 1.0770, G Loss: 0.9910\n",
            "Epoch [22/150] Batch [300/469] D Loss: 1.0963, G Loss: 0.8429\n",
            "Epoch [22/150] Batch [400/469] D Loss: 1.0367, G Loss: 1.3005\n",
            "Epoch [23/150] Batch [0/469] D Loss: 1.0765, G Loss: 0.9697\n",
            "Epoch [23/150] Batch [100/469] D Loss: 1.1566, G Loss: 0.8932\n",
            "Epoch [23/150] Batch [200/469] D Loss: 1.1507, G Loss: 1.1021\n",
            "Epoch [23/150] Batch [300/469] D Loss: 1.2326, G Loss: 0.9333\n",
            "Epoch [23/150] Batch [400/469] D Loss: 1.1904, G Loss: 0.7860\n",
            "Epoch [24/150] Batch [0/469] D Loss: 1.1378, G Loss: 1.0548\n",
            "Epoch [24/150] Batch [100/469] D Loss: 1.2968, G Loss: 1.3263\n",
            "Epoch [24/150] Batch [200/469] D Loss: 1.1043, G Loss: 1.2769\n",
            "Epoch [24/150] Batch [300/469] D Loss: 1.1843, G Loss: 0.9564\n",
            "Epoch [24/150] Batch [400/469] D Loss: 1.2334, G Loss: 1.1708\n",
            "Epoch [25/150] Batch [0/469] D Loss: 1.2092, G Loss: 0.9294\n",
            "Epoch [25/150] Batch [100/469] D Loss: 1.1835, G Loss: 0.6654\n",
            "Epoch [25/150] Batch [200/469] D Loss: 1.4096, G Loss: 1.7774\n",
            "Epoch [25/150] Batch [300/469] D Loss: 1.1739, G Loss: 0.9669\n",
            "Epoch [25/150] Batch [400/469] D Loss: 1.1545, G Loss: 1.2122\n",
            "Epoch [26/150] Batch [0/469] D Loss: 1.2354, G Loss: 1.6651\n",
            "Epoch [26/150] Batch [100/469] D Loss: 1.1562, G Loss: 0.7974\n",
            "Epoch [26/150] Batch [200/469] D Loss: 1.2567, G Loss: 0.7194\n",
            "Epoch [26/150] Batch [300/469] D Loss: 1.1645, G Loss: 1.2253\n",
            "Epoch [26/150] Batch [400/469] D Loss: 1.1220, G Loss: 0.8976\n",
            "Epoch [27/150] Batch [0/469] D Loss: 1.2214, G Loss: 0.9475\n",
            "Epoch [27/150] Batch [100/469] D Loss: 1.2741, G Loss: 0.8093\n",
            "Epoch [27/150] Batch [200/469] D Loss: 1.0967, G Loss: 1.2244\n",
            "Epoch [27/150] Batch [300/469] D Loss: 1.1007, G Loss: 1.1375\n",
            "Epoch [27/150] Batch [400/469] D Loss: 1.2487, G Loss: 1.1831\n",
            "Epoch [28/150] Batch [0/469] D Loss: 1.1149, G Loss: 1.0549\n",
            "Epoch [28/150] Batch [100/469] D Loss: 1.3452, G Loss: 1.8877\n",
            "Epoch [28/150] Batch [200/469] D Loss: 1.1572, G Loss: 0.8236\n",
            "Epoch [28/150] Batch [300/469] D Loss: 1.1843, G Loss: 0.9794\n",
            "Epoch [28/150] Batch [400/469] D Loss: 1.2084, G Loss: 1.1897\n",
            "Epoch [29/150] Batch [0/469] D Loss: 1.1010, G Loss: 1.0692\n",
            "Epoch [29/150] Batch [100/469] D Loss: 1.1820, G Loss: 1.1459\n",
            "Epoch [29/150] Batch [200/469] D Loss: 1.1806, G Loss: 0.9733\n",
            "Epoch [29/150] Batch [300/469] D Loss: 1.1204, G Loss: 1.0780\n",
            "Epoch [29/150] Batch [400/469] D Loss: 1.1980, G Loss: 1.3711\n",
            "Epoch [30/150] Batch [0/469] D Loss: 1.2271, G Loss: 0.7515\n",
            "Epoch [30/150] Batch [100/469] D Loss: 1.0945, G Loss: 0.9832\n",
            "Epoch [30/150] Batch [200/469] D Loss: 1.1621, G Loss: 0.8918\n",
            "Epoch [30/150] Batch [300/469] D Loss: 1.2421, G Loss: 0.9920\n",
            "Epoch [30/150] Batch [400/469] D Loss: 1.0282, G Loss: 1.1665\n",
            "Epoch [31/150] Batch [0/469] D Loss: 1.2240, G Loss: 0.9268\n",
            "Epoch [31/150] Batch [100/469] D Loss: 1.2714, G Loss: 0.9550\n",
            "Epoch [31/150] Batch [200/469] D Loss: 1.1673, G Loss: 0.9823\n",
            "Epoch [31/150] Batch [300/469] D Loss: 1.1273, G Loss: 0.9747\n",
            "Epoch [31/150] Batch [400/469] D Loss: 1.1646, G Loss: 0.8926\n",
            "Epoch [32/150] Batch [0/469] D Loss: 1.1714, G Loss: 1.1429\n",
            "Epoch [32/150] Batch [100/469] D Loss: 1.4555, G Loss: 1.2509\n",
            "Epoch [32/150] Batch [200/469] D Loss: 1.2793, G Loss: 1.3696\n",
            "Epoch [32/150] Batch [300/469] D Loss: 1.1950, G Loss: 1.2808\n",
            "Epoch [32/150] Batch [400/469] D Loss: 1.3874, G Loss: 0.6382\n",
            "Epoch [33/150] Batch [0/469] D Loss: 1.1875, G Loss: 1.0356\n",
            "Epoch [33/150] Batch [100/469] D Loss: 1.1725, G Loss: 0.9850\n",
            "Epoch [33/150] Batch [200/469] D Loss: 1.2071, G Loss: 1.0516\n",
            "Epoch [33/150] Batch [300/469] D Loss: 1.2101, G Loss: 0.8907\n",
            "Epoch [33/150] Batch [400/469] D Loss: 1.0675, G Loss: 1.1115\n",
            "Epoch [34/150] Batch [0/469] D Loss: 1.1372, G Loss: 1.0184\n",
            "Epoch [34/150] Batch [100/469] D Loss: 1.2503, G Loss: 1.0566\n",
            "Epoch [34/150] Batch [200/469] D Loss: 1.1825, G Loss: 1.0120\n",
            "Epoch [34/150] Batch [300/469] D Loss: 1.1931, G Loss: 0.9353\n",
            "Epoch [34/150] Batch [400/469] D Loss: 1.1123, G Loss: 0.9244\n",
            "Epoch [35/150] Batch [0/469] D Loss: 1.1740, G Loss: 0.9928\n",
            "Epoch [35/150] Batch [100/469] D Loss: 1.2376, G Loss: 0.8130\n",
            "Epoch [35/150] Batch [200/469] D Loss: 1.1582, G Loss: 1.0309\n",
            "Epoch [35/150] Batch [300/469] D Loss: 1.2374, G Loss: 1.1468\n",
            "Epoch [35/150] Batch [400/469] D Loss: 1.2366, G Loss: 0.9473\n",
            "Epoch [36/150] Batch [0/469] D Loss: 1.2682, G Loss: 1.1486\n",
            "Epoch [36/150] Batch [100/469] D Loss: 1.3877, G Loss: 1.1317\n",
            "Epoch [36/150] Batch [200/469] D Loss: 1.1388, G Loss: 1.0905\n",
            "Epoch [36/150] Batch [300/469] D Loss: 1.2662, G Loss: 0.9230\n",
            "Epoch [36/150] Batch [400/469] D Loss: 1.2830, G Loss: 0.8241\n",
            "Epoch [37/150] Batch [0/469] D Loss: 1.1777, G Loss: 0.8971\n",
            "Epoch [37/150] Batch [100/469] D Loss: 1.2433, G Loss: 0.7854\n",
            "Epoch [37/150] Batch [200/469] D Loss: 1.1876, G Loss: 1.1930\n",
            "Epoch [37/150] Batch [300/469] D Loss: 1.1980, G Loss: 0.9394\n",
            "Epoch [37/150] Batch [400/469] D Loss: 1.1335, G Loss: 1.0511\n",
            "Epoch [38/150] Batch [0/469] D Loss: 1.2462, G Loss: 0.7518\n",
            "Epoch [38/150] Batch [100/469] D Loss: 1.2717, G Loss: 0.9965\n",
            "Epoch [38/150] Batch [200/469] D Loss: 1.1677, G Loss: 0.8069\n",
            "Epoch [38/150] Batch [300/469] D Loss: 1.2173, G Loss: 0.7108\n",
            "Epoch [38/150] Batch [400/469] D Loss: 1.2686, G Loss: 1.0837\n",
            "Epoch [39/150] Batch [0/469] D Loss: 1.3056, G Loss: 1.0595\n",
            "Epoch [39/150] Batch [100/469] D Loss: 1.2778, G Loss: 0.8769\n",
            "Epoch [39/150] Batch [200/469] D Loss: 1.2082, G Loss: 0.6322\n",
            "Epoch [39/150] Batch [300/469] D Loss: 1.1330, G Loss: 1.1508\n",
            "Epoch [39/150] Batch [400/469] D Loss: 1.2659, G Loss: 0.7663\n",
            "Epoch [40/150] Batch [0/469] D Loss: 1.2149, G Loss: 0.9408\n",
            "Epoch [40/150] Batch [100/469] D Loss: 1.2114, G Loss: 0.8458\n",
            "Epoch [40/150] Batch [200/469] D Loss: 1.3001, G Loss: 0.7784\n",
            "Epoch [40/150] Batch [300/469] D Loss: 1.2219, G Loss: 0.8694\n",
            "Epoch [40/150] Batch [400/469] D Loss: 1.2022, G Loss: 1.0703\n",
            "Epoch [41/150] Batch [0/469] D Loss: 1.2849, G Loss: 0.8351\n",
            "Epoch [41/150] Batch [100/469] D Loss: 1.2559, G Loss: 0.9029\n",
            "Epoch [41/150] Batch [200/469] D Loss: 1.2154, G Loss: 0.8940\n",
            "Epoch [41/150] Batch [300/469] D Loss: 1.1288, G Loss: 1.3288\n",
            "Epoch [41/150] Batch [400/469] D Loss: 1.2125, G Loss: 0.9125\n",
            "Epoch [42/150] Batch [0/469] D Loss: 1.1551, G Loss: 1.0609\n",
            "Epoch [42/150] Batch [100/469] D Loss: 1.1581, G Loss: 0.9649\n",
            "Epoch [42/150] Batch [200/469] D Loss: 1.2068, G Loss: 1.0868\n",
            "Epoch [42/150] Batch [300/469] D Loss: 1.2480, G Loss: 0.9228\n",
            "Epoch [42/150] Batch [400/469] D Loss: 1.2917, G Loss: 0.7621\n",
            "Epoch [43/150] Batch [0/469] D Loss: 1.2664, G Loss: 0.8659\n",
            "Epoch [43/150] Batch [100/469] D Loss: 1.2572, G Loss: 1.0123\n",
            "Epoch [43/150] Batch [200/469] D Loss: 1.1736, G Loss: 0.8651\n",
            "Epoch [43/150] Batch [300/469] D Loss: 1.2459, G Loss: 0.8438\n",
            "Epoch [43/150] Batch [400/469] D Loss: 1.2655, G Loss: 0.9333\n",
            "Epoch [44/150] Batch [0/469] D Loss: 1.2229, G Loss: 0.9373\n",
            "Epoch [44/150] Batch [100/469] D Loss: 1.1976, G Loss: 1.1611\n",
            "Epoch [44/150] Batch [200/469] D Loss: 1.2938, G Loss: 0.7919\n",
            "Epoch [44/150] Batch [300/469] D Loss: 1.2287, G Loss: 0.7779\n",
            "Epoch [44/150] Batch [400/469] D Loss: 1.2682, G Loss: 0.9945\n",
            "Epoch [45/150] Batch [0/469] D Loss: 1.2988, G Loss: 1.3214\n",
            "Epoch [45/150] Batch [100/469] D Loss: 1.2102, G Loss: 0.8201\n",
            "Epoch [45/150] Batch [200/469] D Loss: 1.2743, G Loss: 0.8804\n",
            "Epoch [45/150] Batch [300/469] D Loss: 1.2662, G Loss: 1.1342\n",
            "Epoch [45/150] Batch [400/469] D Loss: 1.1975, G Loss: 0.8977\n",
            "Epoch [46/150] Batch [0/469] D Loss: 1.2025, G Loss: 0.9492\n",
            "Epoch [46/150] Batch [100/469] D Loss: 1.2993, G Loss: 0.7661\n",
            "Epoch [46/150] Batch [200/469] D Loss: 1.1864, G Loss: 0.9595\n",
            "Epoch [46/150] Batch [300/469] D Loss: 1.2696, G Loss: 1.1014\n",
            "Epoch [46/150] Batch [400/469] D Loss: 1.1510, G Loss: 0.9824\n",
            "Epoch [47/150] Batch [0/469] D Loss: 1.1782, G Loss: 0.8300\n",
            "Epoch [47/150] Batch [100/469] D Loss: 1.2421, G Loss: 0.7974\n",
            "Epoch [47/150] Batch [200/469] D Loss: 1.3418, G Loss: 0.8099\n",
            "Epoch [47/150] Batch [300/469] D Loss: 1.2854, G Loss: 0.8295\n",
            "Epoch [47/150] Batch [400/469] D Loss: 1.2125, G Loss: 0.9621\n",
            "Epoch [48/150] Batch [0/469] D Loss: 1.1749, G Loss: 1.1851\n",
            "Epoch [48/150] Batch [100/469] D Loss: 1.4128, G Loss: 0.9313\n",
            "Epoch [48/150] Batch [200/469] D Loss: 1.3348, G Loss: 1.0381\n",
            "Epoch [48/150] Batch [300/469] D Loss: 1.2755, G Loss: 1.1800\n",
            "Epoch [48/150] Batch [400/469] D Loss: 1.2489, G Loss: 0.9831\n",
            "Epoch [49/150] Batch [0/469] D Loss: 1.2348, G Loss: 0.9346\n",
            "Epoch [49/150] Batch [100/469] D Loss: 1.2091, G Loss: 0.9361\n",
            "Epoch [49/150] Batch [200/469] D Loss: 1.1672, G Loss: 0.9632\n",
            "Epoch [49/150] Batch [300/469] D Loss: 1.2992, G Loss: 0.9085\n",
            "Epoch [49/150] Batch [400/469] D Loss: 1.2850, G Loss: 0.6909\n",
            "Epoch [50/150] Batch [0/469] D Loss: 1.2455, G Loss: 1.1120\n",
            "Epoch [50/150] Batch [100/469] D Loss: 1.3895, G Loss: 1.1617\n",
            "Epoch [50/150] Batch [200/469] D Loss: 1.2371, G Loss: 0.7834\n",
            "Epoch [50/150] Batch [300/469] D Loss: 1.3161, G Loss: 0.9743\n",
            "Epoch [50/150] Batch [400/469] D Loss: 1.2678, G Loss: 0.8230\n",
            "Epoch [51/150] Batch [0/469] D Loss: 1.3052, G Loss: 0.8670\n",
            "Epoch [51/150] Batch [100/469] D Loss: 1.2977, G Loss: 1.0797\n",
            "Epoch [51/150] Batch [200/469] D Loss: 1.2607, G Loss: 1.3630\n",
            "Epoch [51/150] Batch [300/469] D Loss: 1.2511, G Loss: 0.9963\n",
            "Epoch [51/150] Batch [400/469] D Loss: 1.3071, G Loss: 0.9513\n",
            "Epoch [52/150] Batch [0/469] D Loss: 1.2472, G Loss: 0.8022\n",
            "Epoch [52/150] Batch [100/469] D Loss: 1.3009, G Loss: 0.7793\n",
            "Epoch [52/150] Batch [200/469] D Loss: 1.2646, G Loss: 0.7363\n",
            "Epoch [52/150] Batch [300/469] D Loss: 1.2161, G Loss: 0.8964\n",
            "Epoch [52/150] Batch [400/469] D Loss: 1.2869, G Loss: 0.8095\n",
            "Epoch [53/150] Batch [0/469] D Loss: 1.3072, G Loss: 1.1147\n",
            "Epoch [53/150] Batch [100/469] D Loss: 1.1846, G Loss: 1.1491\n",
            "Epoch [53/150] Batch [200/469] D Loss: 1.2315, G Loss: 1.0519\n",
            "Epoch [53/150] Batch [300/469] D Loss: 1.2037, G Loss: 0.8300\n",
            "Epoch [53/150] Batch [400/469] D Loss: 1.3533, G Loss: 0.9191\n",
            "Epoch [54/150] Batch [0/469] D Loss: 1.2819, G Loss: 0.7864\n",
            "Epoch [54/150] Batch [100/469] D Loss: 1.1765, G Loss: 0.9850\n",
            "Epoch [54/150] Batch [200/469] D Loss: 1.2654, G Loss: 0.8504\n",
            "Epoch [54/150] Batch [300/469] D Loss: 1.2884, G Loss: 1.0087\n",
            "Epoch [54/150] Batch [400/469] D Loss: 1.2987, G Loss: 0.8975\n",
            "Epoch [55/150] Batch [0/469] D Loss: 1.3276, G Loss: 0.7023\n",
            "Epoch [55/150] Batch [100/469] D Loss: 1.3462, G Loss: 0.7496\n",
            "Epoch [55/150] Batch [200/469] D Loss: 1.2057, G Loss: 0.7217\n",
            "Epoch [55/150] Batch [300/469] D Loss: 1.2019, G Loss: 0.8984\n",
            "Epoch [55/150] Batch [400/469] D Loss: 1.2928, G Loss: 0.6571\n",
            "Epoch [56/150] Batch [0/469] D Loss: 1.2828, G Loss: 0.9688\n",
            "Epoch [56/150] Batch [100/469] D Loss: 1.2699, G Loss: 0.8928\n",
            "Epoch [56/150] Batch [200/469] D Loss: 1.2784, G Loss: 1.0797\n",
            "Epoch [56/150] Batch [300/469] D Loss: 1.3461, G Loss: 1.0290\n",
            "Epoch [56/150] Batch [400/469] D Loss: 1.3495, G Loss: 1.1195\n",
            "Epoch [57/150] Batch [0/469] D Loss: 1.1544, G Loss: 1.0351\n",
            "Epoch [57/150] Batch [100/469] D Loss: 1.2815, G Loss: 0.9657\n",
            "Epoch [57/150] Batch [200/469] D Loss: 1.2905, G Loss: 0.9581\n",
            "Epoch [57/150] Batch [300/469] D Loss: 1.3290, G Loss: 0.8333\n",
            "Epoch [57/150] Batch [400/469] D Loss: 1.2531, G Loss: 1.0189\n",
            "Epoch [58/150] Batch [0/469] D Loss: 1.2480, G Loss: 0.9044\n",
            "Epoch [58/150] Batch [100/469] D Loss: 1.2892, G Loss: 1.0306\n",
            "Epoch [58/150] Batch [200/469] D Loss: 1.3006, G Loss: 0.7945\n",
            "Epoch [58/150] Batch [300/469] D Loss: 1.2377, G Loss: 1.1170\n",
            "Epoch [58/150] Batch [400/469] D Loss: 1.3392, G Loss: 1.0640\n",
            "Epoch [59/150] Batch [0/469] D Loss: 1.3074, G Loss: 0.6906\n",
            "Epoch [59/150] Batch [100/469] D Loss: 1.2824, G Loss: 0.8350\n",
            "Epoch [59/150] Batch [200/469] D Loss: 1.2580, G Loss: 0.9540\n",
            "Epoch [59/150] Batch [300/469] D Loss: 1.3127, G Loss: 0.9236\n",
            "Epoch [59/150] Batch [400/469] D Loss: 1.3112, G Loss: 0.8613\n",
            "Epoch [60/150] Batch [0/469] D Loss: 1.3105, G Loss: 1.0247\n",
            "Epoch [60/150] Batch [100/469] D Loss: 1.2327, G Loss: 0.8453\n",
            "Epoch [60/150] Batch [200/469] D Loss: 1.3399, G Loss: 0.9396\n",
            "Epoch [60/150] Batch [300/469] D Loss: 1.2226, G Loss: 0.8524\n",
            "Epoch [60/150] Batch [400/469] D Loss: 1.2158, G Loss: 0.8384\n",
            "Epoch [61/150] Batch [0/469] D Loss: 1.2631, G Loss: 1.0497\n",
            "Epoch [61/150] Batch [100/469] D Loss: 1.2706, G Loss: 0.7306\n",
            "Epoch [61/150] Batch [200/469] D Loss: 1.3085, G Loss: 0.8401\n",
            "Epoch [61/150] Batch [300/469] D Loss: 1.3501, G Loss: 0.8934\n",
            "Epoch [61/150] Batch [400/469] D Loss: 1.2680, G Loss: 0.7379\n",
            "Epoch [62/150] Batch [0/469] D Loss: 1.2972, G Loss: 0.9672\n",
            "Epoch [62/150] Batch [100/469] D Loss: 1.3630, G Loss: 0.7541\n",
            "Epoch [62/150] Batch [200/469] D Loss: 1.3089, G Loss: 0.7459\n",
            "Epoch [62/150] Batch [300/469] D Loss: 1.2666, G Loss: 0.9462\n",
            "Epoch [62/150] Batch [400/469] D Loss: 1.2980, G Loss: 0.7672\n",
            "Epoch [63/150] Batch [0/469] D Loss: 1.2887, G Loss: 0.8794\n",
            "Epoch [63/150] Batch [100/469] D Loss: 1.2296, G Loss: 0.8287\n",
            "Epoch [63/150] Batch [200/469] D Loss: 1.3216, G Loss: 0.7316\n",
            "Epoch [63/150] Batch [300/469] D Loss: 1.3075, G Loss: 0.7983\n",
            "Epoch [63/150] Batch [400/469] D Loss: 1.2357, G Loss: 0.9362\n",
            "Epoch [64/150] Batch [0/469] D Loss: 1.2532, G Loss: 0.9330\n",
            "Epoch [64/150] Batch [100/469] D Loss: 1.3142, G Loss: 0.9715\n",
            "Epoch [64/150] Batch [200/469] D Loss: 1.3729, G Loss: 0.8836\n",
            "Epoch [64/150] Batch [300/469] D Loss: 1.3153, G Loss: 0.7703\n",
            "Epoch [64/150] Batch [400/469] D Loss: 1.3288, G Loss: 0.9814\n",
            "Epoch [65/150] Batch [0/469] D Loss: 1.2758, G Loss: 0.8030\n",
            "Epoch [65/150] Batch [100/469] D Loss: 1.3505, G Loss: 0.9458\n",
            "Epoch [65/150] Batch [200/469] D Loss: 1.2825, G Loss: 0.8277\n",
            "Epoch [65/150] Batch [300/469] D Loss: 1.3392, G Loss: 0.8326\n",
            "Epoch [65/150] Batch [400/469] D Loss: 1.2520, G Loss: 1.0836\n",
            "Epoch [66/150] Batch [0/469] D Loss: 1.4134, G Loss: 1.0760\n",
            "Epoch [66/150] Batch [100/469] D Loss: 1.2571, G Loss: 0.7964\n",
            "Epoch [66/150] Batch [200/469] D Loss: 1.2211, G Loss: 0.9841\n",
            "Epoch [66/150] Batch [300/469] D Loss: 1.2276, G Loss: 0.9104\n",
            "Epoch [66/150] Batch [400/469] D Loss: 1.3168, G Loss: 0.9183\n",
            "Epoch [67/150] Batch [0/469] D Loss: 1.3265, G Loss: 0.9537\n",
            "Epoch [67/150] Batch [100/469] D Loss: 1.2791, G Loss: 0.8021\n",
            "Epoch [67/150] Batch [200/469] D Loss: 1.2149, G Loss: 0.8978\n",
            "Epoch [67/150] Batch [300/469] D Loss: 1.3460, G Loss: 1.0640\n",
            "Epoch [67/150] Batch [400/469] D Loss: 1.2730, G Loss: 0.8059\n",
            "Epoch [68/150] Batch [0/469] D Loss: 1.3530, G Loss: 0.7887\n",
            "Epoch [68/150] Batch [100/469] D Loss: 1.2045, G Loss: 0.8136\n",
            "Epoch [68/150] Batch [200/469] D Loss: 1.2317, G Loss: 1.0296\n",
            "Epoch [68/150] Batch [300/469] D Loss: 1.2724, G Loss: 0.8294\n",
            "Epoch [68/150] Batch [400/469] D Loss: 1.2343, G Loss: 1.0433\n",
            "Epoch [69/150] Batch [0/469] D Loss: 1.2691, G Loss: 0.7315\n",
            "Epoch [69/150] Batch [100/469] D Loss: 1.3266, G Loss: 0.6174\n",
            "Epoch [69/150] Batch [200/469] D Loss: 1.2328, G Loss: 0.9803\n",
            "Epoch [69/150] Batch [300/469] D Loss: 1.2828, G Loss: 0.7994\n",
            "Epoch [69/150] Batch [400/469] D Loss: 1.3119, G Loss: 1.0691\n",
            "Epoch [70/150] Batch [0/469] D Loss: 1.3238, G Loss: 0.7510\n",
            "Epoch [70/150] Batch [100/469] D Loss: 1.3031, G Loss: 1.0357\n",
            "Epoch [70/150] Batch [200/469] D Loss: 1.2497, G Loss: 0.8004\n",
            "Epoch [70/150] Batch [300/469] D Loss: 1.2774, G Loss: 0.9097\n",
            "Epoch [70/150] Batch [400/469] D Loss: 1.2907, G Loss: 0.8702\n",
            "Epoch [71/150] Batch [0/469] D Loss: 1.2642, G Loss: 1.0805\n",
            "Epoch [71/150] Batch [100/469] D Loss: 1.3582, G Loss: 0.5641\n",
            "Epoch [71/150] Batch [200/469] D Loss: 1.3767, G Loss: 0.7117\n",
            "Epoch [71/150] Batch [300/469] D Loss: 1.2651, G Loss: 1.0928\n",
            "Epoch [71/150] Batch [400/469] D Loss: 1.4194, G Loss: 0.7715\n",
            "Epoch [72/150] Batch [0/469] D Loss: 1.3225, G Loss: 1.0146\n",
            "Epoch [72/150] Batch [100/469] D Loss: 1.3433, G Loss: 0.8677\n",
            "Epoch [72/150] Batch [200/469] D Loss: 1.2388, G Loss: 0.8772\n",
            "Epoch [72/150] Batch [300/469] D Loss: 1.3674, G Loss: 0.9367\n",
            "Epoch [72/150] Batch [400/469] D Loss: 1.3402, G Loss: 0.8234\n",
            "Epoch [73/150] Batch [0/469] D Loss: 1.3074, G Loss: 0.8538\n",
            "Epoch [73/150] Batch [100/469] D Loss: 1.2694, G Loss: 0.8232\n",
            "Epoch [73/150] Batch [200/469] D Loss: 1.3377, G Loss: 0.7124\n",
            "Epoch [73/150] Batch [300/469] D Loss: 1.4301, G Loss: 1.0006\n",
            "Epoch [73/150] Batch [400/469] D Loss: 1.3209, G Loss: 0.8769\n",
            "Epoch [74/150] Batch [0/469] D Loss: 1.2870, G Loss: 0.7489\n",
            "Epoch [74/150] Batch [100/469] D Loss: 1.3417, G Loss: 0.8290\n",
            "Epoch [74/150] Batch [200/469] D Loss: 1.2497, G Loss: 0.9654\n",
            "Epoch [74/150] Batch [300/469] D Loss: 1.3788, G Loss: 0.6689\n",
            "Epoch [74/150] Batch [400/469] D Loss: 1.2844, G Loss: 1.0818\n",
            "Epoch [75/150] Batch [0/469] D Loss: 1.3503, G Loss: 1.0741\n",
            "Epoch [75/150] Batch [100/469] D Loss: 1.2851, G Loss: 0.9843\n",
            "Epoch [75/150] Batch [200/469] D Loss: 1.2736, G Loss: 0.9238\n",
            "Epoch [75/150] Batch [300/469] D Loss: 1.3019, G Loss: 1.0359\n",
            "Epoch [75/150] Batch [400/469] D Loss: 1.3186, G Loss: 0.9134\n",
            "Epoch [76/150] Batch [0/469] D Loss: 1.3235, G Loss: 0.8782\n",
            "Epoch [76/150] Batch [100/469] D Loss: 1.2532, G Loss: 0.7872\n",
            "Epoch [76/150] Batch [200/469] D Loss: 1.3154, G Loss: 0.8018\n",
            "Epoch [76/150] Batch [300/469] D Loss: 1.3380, G Loss: 0.9060\n",
            "Epoch [76/150] Batch [400/469] D Loss: 1.3758, G Loss: 0.9682\n",
            "Epoch [77/150] Batch [0/469] D Loss: 1.3343, G Loss: 0.8275\n",
            "Epoch [77/150] Batch [100/469] D Loss: 1.2858, G Loss: 0.8690\n",
            "Epoch [77/150] Batch [200/469] D Loss: 1.3766, G Loss: 0.6586\n",
            "Epoch [77/150] Batch [300/469] D Loss: 1.3246, G Loss: 0.8407\n",
            "Epoch [77/150] Batch [400/469] D Loss: 1.4224, G Loss: 0.7713\n",
            "Epoch [78/150] Batch [0/469] D Loss: 1.2769, G Loss: 0.7784\n",
            "Epoch [78/150] Batch [100/469] D Loss: 1.2370, G Loss: 0.9347\n",
            "Epoch [78/150] Batch [200/469] D Loss: 1.3318, G Loss: 0.8995\n",
            "Epoch [78/150] Batch [300/469] D Loss: 1.2451, G Loss: 0.8308\n",
            "Epoch [78/150] Batch [400/469] D Loss: 1.2915, G Loss: 0.7710\n",
            "Epoch [79/150] Batch [0/469] D Loss: 1.2986, G Loss: 0.6872\n",
            "Epoch [79/150] Batch [100/469] D Loss: 1.3042, G Loss: 0.8729\n",
            "Epoch [79/150] Batch [200/469] D Loss: 1.2701, G Loss: 0.7759\n",
            "Epoch [79/150] Batch [300/469] D Loss: 1.3336, G Loss: 0.9144\n",
            "Epoch [79/150] Batch [400/469] D Loss: 1.3351, G Loss: 0.9450\n",
            "Epoch [80/150] Batch [0/469] D Loss: 1.2876, G Loss: 0.7784\n",
            "Epoch [80/150] Batch [100/469] D Loss: 1.3099, G Loss: 0.7366\n",
            "Epoch [80/150] Batch [200/469] D Loss: 1.3668, G Loss: 0.8627\n",
            "Epoch [80/150] Batch [300/469] D Loss: 1.3254, G Loss: 0.8683\n",
            "Epoch [80/150] Batch [400/469] D Loss: 1.3142, G Loss: 0.9000\n",
            "Epoch [81/150] Batch [0/469] D Loss: 1.2578, G Loss: 0.8261\n",
            "Epoch [81/150] Batch [100/469] D Loss: 1.3739, G Loss: 0.6366\n",
            "Epoch [81/150] Batch [200/469] D Loss: 1.3277, G Loss: 0.8212\n",
            "Epoch [81/150] Batch [300/469] D Loss: 1.4598, G Loss: 0.7616\n",
            "Epoch [81/150] Batch [400/469] D Loss: 1.3464, G Loss: 0.8039\n",
            "Epoch [82/150] Batch [0/469] D Loss: 1.3317, G Loss: 0.7993\n",
            "Epoch [82/150] Batch [100/469] D Loss: 1.3119, G Loss: 0.9713\n",
            "Epoch [82/150] Batch [200/469] D Loss: 1.3737, G Loss: 0.7627\n",
            "Epoch [82/150] Batch [300/469] D Loss: 1.3423, G Loss: 0.8762\n",
            "Epoch [82/150] Batch [400/469] D Loss: 1.2589, G Loss: 0.7944\n",
            "Epoch [83/150] Batch [0/469] D Loss: 1.3650, G Loss: 0.8306\n",
            "Epoch [83/150] Batch [100/469] D Loss: 1.3687, G Loss: 1.1591\n",
            "Epoch [83/150] Batch [200/469] D Loss: 1.3189, G Loss: 0.9660\n",
            "Epoch [83/150] Batch [300/469] D Loss: 1.2994, G Loss: 0.8402\n",
            "Epoch [83/150] Batch [400/469] D Loss: 1.3214, G Loss: 0.8411\n",
            "Epoch [84/150] Batch [0/469] D Loss: 1.3575, G Loss: 0.9006\n",
            "Epoch [84/150] Batch [100/469] D Loss: 1.3761, G Loss: 0.9042\n",
            "Epoch [84/150] Batch [200/469] D Loss: 1.3302, G Loss: 0.7686\n",
            "Epoch [84/150] Batch [300/469] D Loss: 1.3909, G Loss: 0.7164\n",
            "Epoch [84/150] Batch [400/469] D Loss: 1.3599, G Loss: 0.8076\n",
            "Epoch [85/150] Batch [0/469] D Loss: 1.3882, G Loss: 0.9250\n",
            "Epoch [85/150] Batch [100/469] D Loss: 1.3086, G Loss: 0.7336\n",
            "Epoch [85/150] Batch [200/469] D Loss: 1.3760, G Loss: 0.8071\n",
            "Epoch [85/150] Batch [300/469] D Loss: 1.3565, G Loss: 0.7818\n",
            "Epoch [85/150] Batch [400/469] D Loss: 1.2991, G Loss: 0.8716\n",
            "Epoch [86/150] Batch [0/469] D Loss: 1.3685, G Loss: 1.0104\n",
            "Epoch [86/150] Batch [100/469] D Loss: 1.2735, G Loss: 0.9216\n",
            "Epoch [86/150] Batch [200/469] D Loss: 1.2958, G Loss: 0.7510\n",
            "Epoch [86/150] Batch [300/469] D Loss: 1.3938, G Loss: 0.7008\n",
            "Epoch [86/150] Batch [400/469] D Loss: 1.3389, G Loss: 0.7405\n",
            "Epoch [87/150] Batch [0/469] D Loss: 1.3507, G Loss: 0.8791\n",
            "Epoch [87/150] Batch [100/469] D Loss: 1.3864, G Loss: 0.7415\n",
            "Epoch [87/150] Batch [200/469] D Loss: 1.4779, G Loss: 1.1620\n",
            "Epoch [87/150] Batch [300/469] D Loss: 1.3739, G Loss: 0.9439\n",
            "Epoch [87/150] Batch [400/469] D Loss: 1.3295, G Loss: 0.8703\n",
            "Epoch [88/150] Batch [0/469] D Loss: 1.2922, G Loss: 0.9558\n",
            "Epoch [88/150] Batch [100/469] D Loss: 1.3326, G Loss: 1.0283\n",
            "Epoch [88/150] Batch [200/469] D Loss: 1.3433, G Loss: 0.8315\n",
            "Epoch [88/150] Batch [300/469] D Loss: 1.2326, G Loss: 0.8655\n",
            "Epoch [88/150] Batch [400/469] D Loss: 1.3527, G Loss: 0.8244\n",
            "Epoch [89/150] Batch [0/469] D Loss: 1.3083, G Loss: 0.8416\n",
            "Epoch [89/150] Batch [100/469] D Loss: 1.3834, G Loss: 0.9013\n",
            "Epoch [89/150] Batch [200/469] D Loss: 1.3787, G Loss: 0.7473\n",
            "Epoch [89/150] Batch [300/469] D Loss: 1.3366, G Loss: 0.7235\n",
            "Epoch [89/150] Batch [400/469] D Loss: 1.3498, G Loss: 0.8010\n",
            "Epoch [90/150] Batch [0/469] D Loss: 1.4328, G Loss: 0.6252\n",
            "Epoch [90/150] Batch [100/469] D Loss: 1.3041, G Loss: 0.9050\n",
            "Epoch [90/150] Batch [200/469] D Loss: 1.3745, G Loss: 0.9780\n",
            "Epoch [90/150] Batch [300/469] D Loss: 1.3532, G Loss: 0.6493\n",
            "Epoch [90/150] Batch [400/469] D Loss: 1.3349, G Loss: 0.7225\n",
            "Epoch [91/150] Batch [0/469] D Loss: 1.2735, G Loss: 0.7789\n",
            "Epoch [91/150] Batch [100/469] D Loss: 1.3240, G Loss: 0.9242\n",
            "Epoch [91/150] Batch [200/469] D Loss: 1.3653, G Loss: 0.8772\n",
            "Epoch [91/150] Batch [300/469] D Loss: 1.3569, G Loss: 0.9563\n",
            "Epoch [91/150] Batch [400/469] D Loss: 1.3570, G Loss: 0.8175\n",
            "Epoch [92/150] Batch [0/469] D Loss: 1.3187, G Loss: 0.8265\n",
            "Epoch [92/150] Batch [100/469] D Loss: 1.3862, G Loss: 0.7595\n",
            "Epoch [92/150] Batch [200/469] D Loss: 1.3379, G Loss: 0.6283\n",
            "Epoch [92/150] Batch [300/469] D Loss: 1.3650, G Loss: 0.8611\n",
            "Epoch [92/150] Batch [400/469] D Loss: 1.3370, G Loss: 0.7384\n",
            "Epoch [93/150] Batch [0/469] D Loss: 1.3740, G Loss: 0.6816\n",
            "Epoch [93/150] Batch [100/469] D Loss: 1.3688, G Loss: 0.7353\n",
            "Epoch [93/150] Batch [200/469] D Loss: 1.4108, G Loss: 0.8485\n",
            "Epoch [93/150] Batch [300/469] D Loss: 1.2476, G Loss: 0.9114\n",
            "Epoch [93/150] Batch [400/469] D Loss: 1.3319, G Loss: 0.7901\n",
            "Epoch [94/150] Batch [0/469] D Loss: 1.3636, G Loss: 0.7137\n",
            "Epoch [94/150] Batch [100/469] D Loss: 1.2740, G Loss: 0.8862\n",
            "Epoch [94/150] Batch [200/469] D Loss: 1.3781, G Loss: 0.7458\n",
            "Epoch [94/150] Batch [300/469] D Loss: 1.3486, G Loss: 0.8173\n",
            "Epoch [94/150] Batch [400/469] D Loss: 1.2709, G Loss: 1.0456\n",
            "Epoch [95/150] Batch [0/469] D Loss: 1.3295, G Loss: 0.9340\n",
            "Epoch [95/150] Batch [100/469] D Loss: 1.3241, G Loss: 0.9196\n",
            "Epoch [95/150] Batch [200/469] D Loss: 1.3132, G Loss: 0.8196\n",
            "Epoch [95/150] Batch [300/469] D Loss: 1.3530, G Loss: 0.7882\n",
            "Epoch [95/150] Batch [400/469] D Loss: 1.3187, G Loss: 0.8009\n",
            "Epoch [96/150] Batch [0/469] D Loss: 1.3325, G Loss: 0.8119\n",
            "Epoch [96/150] Batch [100/469] D Loss: 1.3144, G Loss: 0.7612\n",
            "Epoch [96/150] Batch [200/469] D Loss: 1.3327, G Loss: 0.6842\n",
            "Epoch [96/150] Batch [300/469] D Loss: 1.2961, G Loss: 0.9364\n",
            "Epoch [96/150] Batch [400/469] D Loss: 1.2974, G Loss: 0.8203\n",
            "Epoch [97/150] Batch [0/469] D Loss: 1.3498, G Loss: 0.7363\n",
            "Epoch [97/150] Batch [100/469] D Loss: 1.3516, G Loss: 0.7244\n",
            "Epoch [97/150] Batch [200/469] D Loss: 1.3257, G Loss: 0.7488\n",
            "Epoch [97/150] Batch [300/469] D Loss: 1.3061, G Loss: 0.8546\n",
            "Epoch [97/150] Batch [400/469] D Loss: 1.3822, G Loss: 0.8697\n",
            "Epoch [98/150] Batch [0/469] D Loss: 1.3317, G Loss: 0.7960\n",
            "Epoch [98/150] Batch [100/469] D Loss: 1.2906, G Loss: 0.8881\n",
            "Epoch [98/150] Batch [200/469] D Loss: 1.3469, G Loss: 0.8964\n",
            "Epoch [98/150] Batch [300/469] D Loss: 1.3708, G Loss: 0.8553\n",
            "Epoch [98/150] Batch [400/469] D Loss: 1.3929, G Loss: 0.7146\n",
            "Epoch [99/150] Batch [0/469] D Loss: 1.3002, G Loss: 0.8007\n",
            "Epoch [99/150] Batch [100/469] D Loss: 1.3518, G Loss: 0.7460\n",
            "Epoch [99/150] Batch [200/469] D Loss: 1.3259, G Loss: 0.8972\n",
            "Epoch [99/150] Batch [300/469] D Loss: 1.3984, G Loss: 0.8075\n",
            "Epoch [99/150] Batch [400/469] D Loss: 1.3407, G Loss: 0.6931\n",
            "Epoch [100/150] Batch [0/469] D Loss: 1.3881, G Loss: 0.8430\n",
            "Epoch [100/150] Batch [100/469] D Loss: 1.3278, G Loss: 0.8251\n",
            "Epoch [100/150] Batch [200/469] D Loss: 1.3522, G Loss: 0.7730\n",
            "Epoch [100/150] Batch [300/469] D Loss: 1.4132, G Loss: 0.5824\n",
            "Epoch [100/150] Batch [400/469] D Loss: 1.3876, G Loss: 0.7633\n",
            "Epoch [101/150] Batch [0/469] D Loss: 1.4013, G Loss: 0.9330\n",
            "Epoch [101/150] Batch [100/469] D Loss: 1.3632, G Loss: 1.0076\n",
            "Epoch [101/150] Batch [200/469] D Loss: 1.3463, G Loss: 0.7979\n",
            "Epoch [101/150] Batch [300/469] D Loss: 1.3847, G Loss: 0.7638\n",
            "Epoch [101/150] Batch [400/469] D Loss: 1.3434, G Loss: 0.8469\n",
            "Epoch [102/150] Batch [0/469] D Loss: 1.3585, G Loss: 1.0306\n",
            "Epoch [102/150] Batch [100/469] D Loss: 1.3583, G Loss: 0.8058\n",
            "Epoch [102/150] Batch [200/469] D Loss: 1.3682, G Loss: 0.9754\n",
            "Epoch [102/150] Batch [300/469] D Loss: 1.3144, G Loss: 0.7806\n",
            "Epoch [102/150] Batch [400/469] D Loss: 1.3218, G Loss: 0.8723\n",
            "Epoch [103/150] Batch [0/469] D Loss: 1.3776, G Loss: 0.8232\n",
            "Epoch [103/150] Batch [100/469] D Loss: 1.2904, G Loss: 0.8637\n",
            "Epoch [103/150] Batch [200/469] D Loss: 1.2863, G Loss: 0.9008\n",
            "Epoch [103/150] Batch [300/469] D Loss: 1.2833, G Loss: 0.8396\n",
            "Epoch [103/150] Batch [400/469] D Loss: 1.3853, G Loss: 0.7761\n",
            "Epoch [104/150] Batch [0/469] D Loss: 1.3832, G Loss: 0.9998\n",
            "Epoch [104/150] Batch [100/469] D Loss: 1.3103, G Loss: 0.8891\n",
            "Epoch [104/150] Batch [200/469] D Loss: 1.3443, G Loss: 0.7539\n",
            "Epoch [104/150] Batch [300/469] D Loss: 1.3539, G Loss: 0.7620\n",
            "Epoch [104/150] Batch [400/469] D Loss: 1.3727, G Loss: 0.7463\n",
            "Epoch [105/150] Batch [0/469] D Loss: 1.3163, G Loss: 0.8628\n",
            "Epoch [105/150] Batch [100/469] D Loss: 1.3282, G Loss: 0.8720\n",
            "Epoch [105/150] Batch [200/469] D Loss: 1.4346, G Loss: 0.7305\n",
            "Epoch [105/150] Batch [300/469] D Loss: 1.3308, G Loss: 0.9317\n",
            "Epoch [105/150] Batch [400/469] D Loss: 1.2863, G Loss: 0.8793\n",
            "Epoch [106/150] Batch [0/469] D Loss: 1.3615, G Loss: 0.6892\n",
            "Epoch [106/150] Batch [100/469] D Loss: 1.3116, G Loss: 0.7828\n",
            "Epoch [106/150] Batch [200/469] D Loss: 1.2800, G Loss: 0.8547\n",
            "Epoch [106/150] Batch [300/469] D Loss: 1.3758, G Loss: 0.7960\n",
            "Epoch [106/150] Batch [400/469] D Loss: 1.3417, G Loss: 0.8417\n",
            "Epoch [107/150] Batch [0/469] D Loss: 1.3600, G Loss: 1.0260\n",
            "Epoch [107/150] Batch [100/469] D Loss: 1.3390, G Loss: 0.8365\n",
            "Epoch [107/150] Batch [200/469] D Loss: 1.3904, G Loss: 0.6422\n",
            "Epoch [107/150] Batch [300/469] D Loss: 1.3814, G Loss: 0.7146\n",
            "Epoch [107/150] Batch [400/469] D Loss: 1.3889, G Loss: 0.8319\n",
            "Epoch [108/150] Batch [0/469] D Loss: 1.3584, G Loss: 0.8199\n",
            "Epoch [108/150] Batch [100/469] D Loss: 1.4258, G Loss: 0.8152\n",
            "Epoch [108/150] Batch [200/469] D Loss: 1.3772, G Loss: 0.8432\n",
            "Epoch [108/150] Batch [300/469] D Loss: 1.3639, G Loss: 0.7482\n",
            "Epoch [108/150] Batch [400/469] D Loss: 1.2627, G Loss: 0.8129\n",
            "Epoch [109/150] Batch [0/469] D Loss: 1.3754, G Loss: 0.6711\n",
            "Epoch [109/150] Batch [100/469] D Loss: 1.2647, G Loss: 0.8966\n",
            "Epoch [109/150] Batch [200/469] D Loss: 1.2530, G Loss: 0.8485\n",
            "Epoch [109/150] Batch [300/469] D Loss: 1.3945, G Loss: 0.8195\n",
            "Epoch [109/150] Batch [400/469] D Loss: 1.3171, G Loss: 0.8129\n",
            "Epoch [110/150] Batch [0/469] D Loss: 1.3660, G Loss: 0.9094\n",
            "Epoch [110/150] Batch [100/469] D Loss: 1.3469, G Loss: 0.8529\n",
            "Epoch [110/150] Batch [200/469] D Loss: 1.3470, G Loss: 0.8253\n",
            "Epoch [110/150] Batch [300/469] D Loss: 1.2976, G Loss: 0.8243\n",
            "Epoch [110/150] Batch [400/469] D Loss: 1.2866, G Loss: 0.8261\n",
            "Epoch [111/150] Batch [0/469] D Loss: 1.3078, G Loss: 0.8638\n",
            "Epoch [111/150] Batch [100/469] D Loss: 1.3078, G Loss: 0.8397\n",
            "Epoch [111/150] Batch [200/469] D Loss: 1.3665, G Loss: 0.7643\n",
            "Epoch [111/150] Batch [300/469] D Loss: 1.3848, G Loss: 0.9168\n",
            "Epoch [111/150] Batch [400/469] D Loss: 1.3937, G Loss: 0.8006\n",
            "Epoch [112/150] Batch [0/469] D Loss: 1.2821, G Loss: 0.7980\n",
            "Epoch [112/150] Batch [100/469] D Loss: 1.3914, G Loss: 0.7516\n",
            "Epoch [112/150] Batch [200/469] D Loss: 1.3307, G Loss: 0.6764\n",
            "Epoch [112/150] Batch [300/469] D Loss: 1.3424, G Loss: 0.9681\n",
            "Epoch [112/150] Batch [400/469] D Loss: 1.3431, G Loss: 0.8984\n",
            "Epoch [113/150] Batch [0/469] D Loss: 1.3884, G Loss: 0.6937\n",
            "Epoch [113/150] Batch [100/469] D Loss: 1.3056, G Loss: 0.7615\n",
            "Epoch [113/150] Batch [200/469] D Loss: 1.3198, G Loss: 0.9008\n",
            "Epoch [113/150] Batch [300/469] D Loss: 1.3562, G Loss: 0.7781\n",
            "Epoch [113/150] Batch [400/469] D Loss: 1.3494, G Loss: 0.7876\n",
            "Epoch [114/150] Batch [0/469] D Loss: 1.3437, G Loss: 0.9450\n",
            "Epoch [114/150] Batch [100/469] D Loss: 1.3258, G Loss: 0.7340\n",
            "Epoch [114/150] Batch [200/469] D Loss: 1.3466, G Loss: 0.7623\n",
            "Epoch [114/150] Batch [300/469] D Loss: 1.4167, G Loss: 0.8088\n",
            "Epoch [114/150] Batch [400/469] D Loss: 1.3215, G Loss: 0.8928\n",
            "Epoch [115/150] Batch [0/469] D Loss: 1.3479, G Loss: 1.0088\n",
            "Epoch [115/150] Batch [100/469] D Loss: 1.3194, G Loss: 0.7856\n",
            "Epoch [115/150] Batch [200/469] D Loss: 1.3537, G Loss: 0.8383\n",
            "Epoch [115/150] Batch [300/469] D Loss: 1.3532, G Loss: 1.0032\n",
            "Epoch [115/150] Batch [400/469] D Loss: 1.3419, G Loss: 0.8049\n",
            "Epoch [116/150] Batch [0/469] D Loss: 1.3784, G Loss: 0.8887\n",
            "Epoch [116/150] Batch [100/469] D Loss: 1.3419, G Loss: 0.8361\n",
            "Epoch [116/150] Batch [200/469] D Loss: 1.3394, G Loss: 0.9614\n",
            "Epoch [116/150] Batch [300/469] D Loss: 1.3088, G Loss: 0.7567\n",
            "Epoch [116/150] Batch [400/469] D Loss: 1.3773, G Loss: 0.8115\n",
            "Epoch [117/150] Batch [0/469] D Loss: 1.3034, G Loss: 0.7878\n",
            "Epoch [117/150] Batch [100/469] D Loss: 1.3721, G Loss: 0.7257\n",
            "Epoch [117/150] Batch [200/469] D Loss: 1.3887, G Loss: 0.8633\n",
            "Epoch [117/150] Batch [300/469] D Loss: 1.3704, G Loss: 0.8985\n",
            "Epoch [117/150] Batch [400/469] D Loss: 1.2621, G Loss: 0.9384\n",
            "Epoch [118/150] Batch [0/469] D Loss: 1.3225, G Loss: 0.7258\n",
            "Epoch [118/150] Batch [100/469] D Loss: 1.3589, G Loss: 0.8585\n",
            "Epoch [118/150] Batch [200/469] D Loss: 1.2647, G Loss: 0.9444\n",
            "Epoch [118/150] Batch [300/469] D Loss: 1.3510, G Loss: 0.9216\n",
            "Epoch [118/150] Batch [400/469] D Loss: 1.3517, G Loss: 0.8992\n",
            "Epoch [119/150] Batch [0/469] D Loss: 1.3283, G Loss: 0.7458\n",
            "Epoch [119/150] Batch [100/469] D Loss: 1.3792, G Loss: 0.7402\n",
            "Epoch [119/150] Batch [200/469] D Loss: 1.3574, G Loss: 0.7224\n",
            "Epoch [119/150] Batch [300/469] D Loss: 1.3705, G Loss: 0.8459\n",
            "Epoch [119/150] Batch [400/469] D Loss: 1.3971, G Loss: 0.6779\n",
            "Epoch [120/150] Batch [0/469] D Loss: 1.2798, G Loss: 0.8803\n",
            "Epoch [120/150] Batch [100/469] D Loss: 1.3370, G Loss: 0.8122\n",
            "Epoch [120/150] Batch [200/469] D Loss: 1.3017, G Loss: 0.8949\n",
            "Epoch [120/150] Batch [300/469] D Loss: 1.3569, G Loss: 0.8995\n",
            "Epoch [120/150] Batch [400/469] D Loss: 1.3730, G Loss: 0.7363\n",
            "Epoch [121/150] Batch [0/469] D Loss: 1.3310, G Loss: 0.7945\n",
            "Epoch [121/150] Batch [100/469] D Loss: 1.4020, G Loss: 0.6761\n",
            "Epoch [121/150] Batch [200/469] D Loss: 1.3182, G Loss: 0.7947\n",
            "Epoch [121/150] Batch [300/469] D Loss: 1.2726, G Loss: 0.8085\n",
            "Epoch [121/150] Batch [400/469] D Loss: 1.3600, G Loss: 0.7983\n",
            "Epoch [122/150] Batch [0/469] D Loss: 1.3401, G Loss: 0.7295\n",
            "Epoch [122/150] Batch [100/469] D Loss: 1.3088, G Loss: 0.7880\n",
            "Epoch [122/150] Batch [200/469] D Loss: 1.3471, G Loss: 0.7325\n",
            "Epoch [122/150] Batch [300/469] D Loss: 1.3847, G Loss: 0.7811\n",
            "Epoch [122/150] Batch [400/469] D Loss: 1.4073, G Loss: 0.7191\n",
            "Epoch [123/150] Batch [0/469] D Loss: 1.3937, G Loss: 0.8263\n",
            "Epoch [123/150] Batch [100/469] D Loss: 1.3244, G Loss: 0.8406\n",
            "Epoch [123/150] Batch [200/469] D Loss: 1.3217, G Loss: 0.7597\n",
            "Epoch [123/150] Batch [300/469] D Loss: 1.3755, G Loss: 0.9147\n",
            "Epoch [123/150] Batch [400/469] D Loss: 1.3401, G Loss: 0.8691\n",
            "Epoch [124/150] Batch [0/469] D Loss: 1.3211, G Loss: 0.8115\n",
            "Epoch [124/150] Batch [100/469] D Loss: 1.3801, G Loss: 0.7170\n",
            "Epoch [124/150] Batch [200/469] D Loss: 1.3367, G Loss: 0.8753\n",
            "Epoch [124/150] Batch [300/469] D Loss: 1.3482, G Loss: 0.8954\n",
            "Epoch [124/150] Batch [400/469] D Loss: 1.3574, G Loss: 0.6363\n",
            "Epoch [125/150] Batch [0/469] D Loss: 1.2636, G Loss: 0.9102\n",
            "Epoch [125/150] Batch [100/469] D Loss: 1.3309, G Loss: 0.8473\n",
            "Epoch [125/150] Batch [200/469] D Loss: 1.3420, G Loss: 0.6937\n",
            "Epoch [125/150] Batch [300/469] D Loss: 1.3277, G Loss: 0.8789\n",
            "Epoch [125/150] Batch [400/469] D Loss: 1.3669, G Loss: 0.7899\n",
            "Epoch [126/150] Batch [0/469] D Loss: 1.3026, G Loss: 0.8662\n",
            "Epoch [126/150] Batch [100/469] D Loss: 1.3179, G Loss: 0.9080\n",
            "Epoch [126/150] Batch [200/469] D Loss: 1.3679, G Loss: 0.7044\n",
            "Epoch [126/150] Batch [300/469] D Loss: 1.3322, G Loss: 0.9088\n",
            "Epoch [126/150] Batch [400/469] D Loss: 1.3664, G Loss: 0.8410\n",
            "Epoch [127/150] Batch [0/469] D Loss: 1.3306, G Loss: 0.7252\n",
            "Epoch [127/150] Batch [100/469] D Loss: 1.3609, G Loss: 0.7658\n",
            "Epoch [127/150] Batch [200/469] D Loss: 1.3067, G Loss: 0.7454\n",
            "Epoch [127/150] Batch [300/469] D Loss: 1.3513, G Loss: 0.8581\n",
            "Epoch [127/150] Batch [400/469] D Loss: 1.3496, G Loss: 0.7491\n",
            "Epoch [128/150] Batch [0/469] D Loss: 1.3461, G Loss: 0.8030\n",
            "Epoch [128/150] Batch [100/469] D Loss: 1.3775, G Loss: 0.7959\n",
            "Epoch [128/150] Batch [200/469] D Loss: 1.3617, G Loss: 0.7884\n",
            "Epoch [128/150] Batch [300/469] D Loss: 1.3941, G Loss: 0.8289\n",
            "Epoch [128/150] Batch [400/469] D Loss: 1.2971, G Loss: 0.8936\n",
            "Epoch [129/150] Batch [0/469] D Loss: 1.3486, G Loss: 0.9111\n",
            "Epoch [129/150] Batch [100/469] D Loss: 1.3882, G Loss: 0.7760\n",
            "Epoch [129/150] Batch [200/469] D Loss: 1.3995, G Loss: 0.7172\n",
            "Epoch [129/150] Batch [300/469] D Loss: 1.3475, G Loss: 0.8696\n",
            "Epoch [129/150] Batch [400/469] D Loss: 1.3656, G Loss: 0.7974\n",
            "Epoch [130/150] Batch [0/469] D Loss: 1.4047, G Loss: 0.9638\n",
            "Epoch [130/150] Batch [100/469] D Loss: 1.3411, G Loss: 0.7724\n",
            "Epoch [130/150] Batch [200/469] D Loss: 1.3468, G Loss: 0.7499\n",
            "Epoch [130/150] Batch [300/469] D Loss: 1.3888, G Loss: 0.7624\n",
            "Epoch [130/150] Batch [400/469] D Loss: 1.3305, G Loss: 0.7627\n",
            "Epoch [131/150] Batch [0/469] D Loss: 1.3887, G Loss: 0.7662\n",
            "Epoch [131/150] Batch [100/469] D Loss: 1.4000, G Loss: 0.6417\n",
            "Epoch [131/150] Batch [200/469] D Loss: 1.3004, G Loss: 0.8981\n",
            "Epoch [131/150] Batch [300/469] D Loss: 1.4190, G Loss: 0.8980\n",
            "Epoch [131/150] Batch [400/469] D Loss: 1.3500, G Loss: 0.7670\n",
            "Epoch [132/150] Batch [0/469] D Loss: 1.3673, G Loss: 0.7149\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0e826c97594a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms,datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "hidden_dim = 64\n",
        "image_dim = 3  # CIFAR-10: 3 channels (RGB)\n",
        "num_epochs = 150\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.noise_dim = noise_dim\n",
        "\n",
        "        # Initial dense layer to create 7x7x128 feature map\n",
        "        self.fc = nn.Linear(noise_dim, 7 * 7 * 128)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Upsample to 14x14x64\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Upsample to 28x28x32\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Final layer to 28x28x1\n",
        "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()  # Output in [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, noise_dim]\n",
        "        x = self.fc(x)  # [batch_size, 5*5*512]\n",
        "        x = x.view(-1, 128, 7, 7)  # Reshape to [batch_size, 512, 5, 5]\n",
        "        return self.model(x)  # Pass through convolutional layers\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # [batch_size, 64, 100, 100]\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # [batch_size, 128, 50, 50]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*7*7, 1)  # Logit output for real/fake\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Loss and optimizers\n",
        "criterion = nn.BCELoss()\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# Data loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # [0, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # [-1, 1]\n",
        "])\n",
        "dataset = datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, _) in enumerate(dataloader):  # Unpack tuple, ignore labels\n",
        "        real_images = real_images.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # Labels\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        d_optimizer.zero_grad()\n",
        "        real_output = discriminator(real_images)\n",
        "        d_real_loss = criterion(real_output, real_labels)\n",
        "        noise = torch.randn(batch_size, 100).to(device)\n",
        "        fake_images = generator(noise)\n",
        "        fake_output = discriminator(fake_images.detach())\n",
        "        d_fake_loss = criterion(fake_output, fake_labels)\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        g_optimizer.zero_grad()\n",
        "        fake_output = discriminator(fake_images)\n",
        "        g_loss = criterion(fake_output, real_labels)\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(dataloader)}] \"\n",
        "                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "       # Inside the training loop, after each epoch\n",
        "    if epoch % 5 == 0:\n",
        "        torch.save(generator.state_dict(), f'generator_epoch_{epoch}.pth')\n",
        "        torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch}.pth')\n",
        "\n",
        "    # Save generated images\n",
        "    if epoch % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            fake_images = generator(torch.randn(16, 100).to(device))\n",
        "            fake_images = fake_images * 0.5 + 0.5  # Denormalize to [0, 1]\n",
        "            fake_images = fake_images.permute(0, 2, 3, 1).cpu().numpy()\n",
        "            fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "            for i, ax in enumerate(axes.flat):\n",
        "                ax.imshow(fake_images[i])\n",
        "                ax.axis('off')\n",
        "            plt.savefig(f'image_at_epoch_{epoch:04d}.png')\n",
        "            plt.close()"
      ]
    }
  ]
}